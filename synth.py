# This file just has an MVP function, a whole bunch of functions and testing things that'll be refactored.

import ffmpeg
import srt
from TTS.api import TTS
import numpy as np
import re
import Voice
from pydub.playback import play
from pydub import AudioSegment
from audiotsm import wsola
from audiotsm.io.wav import WavReader, WavWriter
from yt_dlp import YoutubeDL

test_video_name = "saiki.mkv"
default_sample_path = "./output/sample.wav"
start_time = 94
end_time =  1324 #1324
remove_xml = re.compile(r'<[^>]+>|\{[^}]+\}')
subs = subs_adjusted = speech_diary = speech_diary_adjusted = []

def load_subs(file):
	output = f"output/{file.split('/')[-1].split('.')[0]}.srt"
	(
		ffmpeg
		.input(file)
		.output(output)
		.global_args('-loglevel', 'error')
		.run(overwrite_output=True)
	)
	with open(output, "r") as f:
		subs = list(srt.parse(f.read()))
		for sub in subs:
			sub.content = re.sub(remove_xml, '', sub.content)
		return subs

# Read RTTM files generated by Pyannote into an array containing the speaker, start, and end of their speech in the audio
def read_diary(file):
	global speakers
	speech_diary = []
	with open(file, 'r') as diary_file:
		for line in diary_file.read().strip().split('\n'):
			line_values = line.split(' ')
			speech_diary.append([line_values[7], float(line_values[3]), float(line_values[4])])
	total_speakers = len(set(line[0] for line in speech_diary)) # determine the total number of speakers in the diary
	speakers = initialize_speakers(total_speakers)

# This function finds the closest index for the given value
def find_nearest(array, value):
	return (np.abs(np.asarray(array) - value)).argmin()

def initialize_speakers(speaker_count):
	speakers = []
	for i in range(total_speakers):
		speakers.append(Voice.SAPI5Voice([], f"Voice {i}"))
	return speakers

def synth():
	for sub in subs_adjusted:
		text = re.sub(remove_xml, '', sub.content)
		# ðŸ¤” How the FRICK does this line work? ðŸ¤”
		current_speaker = int(speech_diary_adjusted[find_nearest([line[1] for line in speech_diary_adjusted], sub.start.total_seconds())][0].split('_')[1])
		current_speaker.set_speed(60*int((len(text.split(' ')) / (sub.end.total_seconds() - sub.start.total_seconds()))))
		file_name = f"files/{sub.index}.wav"
		current_speaker.speak(text, file_name)
		print(text)
		empty_audio = empty_audio.overlay(AudioSegment.from_file(file_name), position=sub.start.total_seconds()*1000)
	empty_audio.export("new.wav")

def sampleVoice(text, output=default_sample_path):
	play(AudioSegment.from_file(sampleSpeaker.speak(text, output)))

def adjust_fit_rate(target_path, source_duration, destination_path=None):
	if destination_path == None:
		destination_path = target_path.split('.')[0] + '-timeshift.wav'
	duration = float(ffmpeg.probe(target_path)["format"]["duration"])
	sound = AudioSegment.from_wav(target_path)
	rate = duration*1/source_duration
	with WavReader(target_path) as reader:
		print(duration, reader.samplerate* reader.samplewidth)
		with WavWriter(destination_path, reader.channels, reader.samplerate) as writer:
			tsm = wsola(reader.channels, speed=rate)
			tsm.run(reader, writer)
	# speedup(sound, rate, 30, 50).export(destination_path, format="wav")
	# ffmpeg.input(target_path).filter('atempo', rate).output(destination_path).run(overwrite_output=True)
	return destination_path

def Download(link):
	YoutubeDL().download(link)
# Download("https://www.youtube.com/watch?v=VOjAlLoXOhQ")

def get_snippet(start, end):
	return current_audio[start*1000:end*1000]

def match_volume(source_snippet, target_path, output_path=None):
	if output_path == None:
		output_path = target_path
	target = AudioSegment.from_file(target_path)
	ratio = source_snippet.rms / target.rms
	# ratio = source_snippet.dBFS - target.dBFS
	adjusted_audio = target.apply_gain(ratio)
	# adjusted_audio = target + raio
	adjusted_audio.export(output_path, format="wav")
	return output_path

def timecode_to_seconds(timecode):
	parts = list(map(float, timecode.split(':')))
	seconds = parts[-1]
	if len(parts) > 1:
		seconds += parts[-2] * 60
	if len(parts) > 2:
		seconds += parts[-3] * 3600
	return seconds

def seconds_to_timecode(seconds):
	hours = int(seconds // 3600)
	minutes = int((seconds % 3600) // 60)
	seconds = seconds % 60
	timecode = ""
	if hours:
		timecode += f"{hours}:"
	if minutes:
		timecode += f"{minutes}:" 
	timecode = f"{timecode}{seconds:.2f}"
	return timecode

def list_streams():
	return [stream for stream in ffmpeg.probe(test_video_name)["streams"] if stream['codec_type'] != 'attachment']

# This is like REALLY STINKY and DESPERATELY needs to be refactored... but i don't wanna right now
def load_video(video_path=test_video_name):
	global subs, current_audio
	subs = load_subs(video_path)
	current_audio = AudioSegment.from_file(video_path)
	time_change(0, float(ffmpeg.probe(video_path)["format"]["duration"]))

def time_change(start, end):
	global speech_diary_adjusted, start_time, end_time, subs_adjusted
	print(start, end)
	start_time = start
	end_time = end
	total_duration = (end - start)*1000
	# clamp the subs to the crop time specified
	start_line = find_nearest([sub.start.total_seconds() for sub in subs], start)
	end_line = find_nearest([sub.start.total_seconds() for sub in subs], end)
	subs_adjusted = subs[start_line:end_line]
	if speech_diary:
		# Time Shift the speech diary to be in line with the start time
		speech_diary_adjusted = [[line[0], line[1] + start, line[2]] for line in speech_diary]
		# adjust speech diary to crop only the bounded lines
		speech_diary_adjusted = speech_diary_adjusted[find_nearest([line[1] for line in speech_diary_adjusted], start):find_nearest([line[1] for line in speech_diary_adjusted], end_line)]

speakers = [Voice.Voice(Voice.Voice.VoiceType.COQUI)]
speakers[0].set_voice_params('tts_models/en/vctk/vits', 'p326') # p340
currentSpeaker = speakers[0]
sampleSpeaker = currentSpeaker
