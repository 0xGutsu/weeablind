# This file implements all the major globals & functions of the app. It's becoming unmaintainable and desperately needs to be refactored.

import ffmpeg
import srt
from TTS.api import TTS
import numpy as np
import re
import Voice
from pydub.playback import play
from pydub import AudioSegment
from audiotsm import wsola
from audiotsm.io.wav import WavReader, WavWriter
from audiotsm.io.array import ArrayReader, ArrayWriter
from yt_dlp import YoutubeDL
from pyannote.audio import Pipeline


test_video_name = "saiki.mkv"
default_sample_path = "./output/sample.wav"
start_time = end_time = total_duration = 0
test_start_time = 94
test_end_time =  1324
remove_xml = re.compile(r'<[^>]+>|\{[^}]+\}')
subs = subs_adjusted = speech_diary = speech_diary_adjusted = []
# pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization@2.1", use_auth_token="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")
current_file = test_video_name

def get_output_path(input, suffix, prefix=''):
	return f"output/{prefix}{input.split('/')[-1].split('.')[0]}{suffix}"

def load_subs(file):
	output = f"output/{file.split('/')[-1].split('.')[0]}.srt"
	(
		ffmpeg
		.input(file)
		.output(output)
		.global_args('-loglevel', 'error')
		.run(overwrite_output=True)
	)
	with open(output, "r") as f:
		subs = list(srt.parse(f.read()))
		for sub in subs:
			sub.content = re.sub(remove_xml, '', sub.content)
			sub.start = sub.start.total_seconds()
			sub.end = sub.end.total_seconds()
		return subs

# Read RTTM files generated by Pyannote into an array containing the speaker, start, and end of their speech in the audio
def load_diary(file):
	global speakers, speech_diary
	speech_diary = []
	with open(file, 'r') as diary_file:
		for line in diary_file.read().strip().split('\n'):
			line_values = line.split(' ')
			speech_diary.append([line_values[7], float(line_values[3]), float(line_values[4])])
	total_speakers = len(set(line[0] for line in speech_diary)) # determine the total number of speakers in the diary
	speakers = initialize_speakers(total_speakers)

def update_diary_timing():
	global speech_diary_adjusted
	# Time Shift the speech diary to be in line with the start time
	speech_diary_adjusted = [[int(line[0].split('_')[1]), line[1] + start_time, line[2]] for line in speech_diary]
	# adjust speech diary to crop only the bounded lines
	# speech_diary_adjusted = speech_diary_adjusted[find_nearest([line[1] for line in speech_diary_adjusted], start_time):find_nearest([line[1] for line in speech_diary_adjusted], end_line)]

# This function finds the closest index for the given value
def find_nearest(array, value):
	return (np.abs(np.asarray(array) - value)).argmin()

def initialize_speakers(speaker_count):
	speakers = []
	for i in range(speaker_count):
		speakers.append(Voice.CoquiVoice([], f"Voice {i}"))
		speakers[i].set_voice_params('tts_models/en/vctk/vits', 'p326') # p340

	return speakers

def post_process(file, source_start, source_end, adjust_volume=True):
	adjustment = match_rate(file, source_end-source_start)
	if adjust_volume:
		adjustment = match_volume(get_snippet(source_start, source_end), adjustment, adjustment)
	return adjustment

def find_nearest_speaker(sub):
	return speech_diary_adjusted[
		find_nearest(
			[diary_entry[1] for diary_entry in speech_diary_adjusted],
			sub.start
		)
	][0]

def run_dubbing():
	empty_audio = AudioSegment.empty()
	for sub in subs_adjusted:
		print(f"{sub.index}/{len(subs_adjusted)}")
		current_speaker = find_nearest_speaker(sub)
		try:
			line = dub_line_ram(current_speaker, sub)
			empty_audio.overlay(line, position=sub.start*1000)
		except:
			pass
	empty_audio.export("new.wav", format="wav")

# This may be used for multithreading?
def combine_segments():
	empty_audio = AudioSegment.silent(total_duration * 1000, frame_rate=22050)
	total_errors = 0
	for sub in subs_adjusted:
		print(f"{sub.index}/{len(subs_adjusted)}")
		try:
			segment = AudioSegment.from_file(f'output/files/{sub.index}.wav')
			empty_audio = empty_audio.overlay(segment, sub.start*1000)
		except:
			total_errors += 1
	empty_audio.export('new.wav')
	print(total_errors)

def sampleVoice(text, output=default_sample_path):
	play(AudioSegment.from_file(sampleSpeaker.speak(text, output)))

def match_rate(target, source_duration, destination_path=None):
	if destination_path == None:
		destination_path = target.split('.')[0] + '-timeshift.wav'
	duration = float(ffmpeg.probe(target)["format"]["duration"])
	rate = duration*1/source_duration
	with WavReader(target) as reader:
		with WavWriter(destination_path, reader.channels, reader.samplerate) as writer:
			tsm = wsola(reader.channels, speed=rate)
			tsm.run(reader, writer)
	return destination_path

def match_rate_ram(target, source_duration):
	num_samples = len(target)
	target = target.reshape(1, num_samples)
	duration = num_samples / 22050
	rate = duration*1/source_duration
	reader = ArrayReader(target)
	tsm = wsola(reader.channels, speed=rate)
	rate_adjusted = ArrayWriter(channels=1)
	tsm.run(reader, rate_adjusted)
	return rate_adjusted.data

def Download(link):
	YoutubeDL().download(link)
# Download("https://www.youtube.com/watch?v=VOjAlLoXOhQ")

def get_snippet(start, end):
	return current_audio[start*1000:end*1000]

def match_volume(source_snippet, target):
	ratio = source_snippet.rms / target.rms
	# ratio = source_snippet.dBFS - target.dBFS
	adjusted_audio = target.apply_gain(ratio)
	# adjusted_audio = target + raio
	return adjusted_audio
	# adjusted_audio.export(output_path, format="wav")

def timecode_to_seconds(timecode):
	parts = list(map(float, timecode.split(':')))
	seconds = parts[-1]
	if len(parts) > 1:
		seconds += parts[-2] * 60
	if len(parts) > 2:
		seconds += parts[-3] * 3600
	return seconds

def seconds_to_timecode(seconds):
	hours = int(seconds // 3600)
	minutes = int((seconds % 3600) // 60)
	seconds = seconds % 60
	timecode = ""
	if hours:
		timecode += f"{hours}:"
	if minutes:
		timecode += f"{minutes}:" 
	timecode = f"{timecode}{seconds:.2f}"
	return timecode

def list_streams():
	return [stream for stream in ffmpeg.probe(test_video_name)["streams"] if stream['codec_type'] != 'attachment']

# This is like REALLY STINKY and DESPERATELY needs to be refactored... but i don't wanna right now
def load_video(video_path=test_video_name):
	global subs, current_audio, total_duration
	subs = load_subs(video_path)
	current_audio = AudioSegment.from_file(video_path)
	total_duration = float(ffmpeg.probe(video_path)["format"]["duration"])
	time_change(0, total_duration)

def time_change(start, end):
	global start_time, end_time, subs_adjusted
	start_time = start
	end_time = end
	# clamp the subs to the crop time specified
	start_line = find_nearest([sub.start for sub in subs], start)
	end_line = find_nearest([sub.start for sub in subs], end)
	subs_adjusted = subs[start_line:end_line]
	if speech_diary:
		update_diary_timing()

def crop_audio(file):
	# ffmpeg -i .\saiki.mkv -vn -ss 84 -to 1325 crop.wav
	output = get_output_path(file, "-crop.wav")
	print(output)
	(
		ffmpeg
		.input(file, ss=start_time, to=end_time)
		.output(output)
		.global_args('-loglevel', 'error')
		.global_args('-vn')
		.run(overwrite_output=True)
	)
	return output

def dub_line_ram(speaker, sub):
	output_path = get_output_path(str(sub.index), '.wav', 'files/')
	tts_audio = speakers[speaker].speak(sub.content, None)
	rate_adjusted = match_rate_ram(tts_audio, sub.end-sub.start)
	audio_as_int = (rate_adjusted * 2**15-1).astype(np.int16).tobytes()
	segment = AudioSegment(
		audio_as_int,
		frame_rate=22050,
		sample_width=2,
		channels=1
	)
	result = match_volume(get_snippet(sub.start, sub.end), segment)
	result.export(output_path, format='wav')
	return result

def run_diarization():
	crop = crop_audio(current_file)
	output = get_output_path(current_file, ".rttm")
	# diarization = pipeline(crop)
	# with open(output, "w") as rttm:
	# 	diarization.write_rttm(rttm)
	load_diary(output)
	update_diary_timing()

speakers = [Voice.Voice(Voice.Voice.VoiceType.COQUI, name="Sample")]
speakers[0].set_voice_params('tts_models/en/vctk/vits', 'p326') # p340
currentSpeaker = speakers[0]
sampleSpeaker = currentSpeaker

load_video(test_video_name)
time_change(test_start_time, test_end_time)
combine_segments()