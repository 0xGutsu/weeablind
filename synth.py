# This file implements all the major globals & functions of the app. It's becoming unmaintainable and desperately needs to be refactored.

import ffmpeg
import numpy as np
import Voice
from pydub.playback import play
from pydub import AudioSegment
from audiotsm import wsola
from audiotsm.io.wav import WavReader, WavWriter
from audiotsm.io.array import ArrayReader, ArrayWriter
from yt_dlp import YoutubeDL
from pyannote.audio import Pipeline
import time
import concurrent.futures
from speechbrain.pretrained import EncoderClassifier
from dub_line import load_subs
import random
import torchaudio
import torchaudio.transforms as T
import os.path

test_video_name = "./output/download.webm"
default_sample_path = "./output/sample.wav"
start_time = end_time = total_duration = 0
test_start_time = 94
test_end_time =  1324
subs = subs_adjusted = speech_diary = speech_diary_adjusted = []
# pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization@2.1", use_auth_token="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")
current_file = test_video_name
dub_track = None
is_video_multi_speaker = False


def get_output_path(input, suffix, prefix='', path=''):
	filename = os.path.basename(input)
	filename_without_extension = os.path.splitext(filename)[0]
	return os.path.join('output', path, f"{prefix}{filename_without_extension}{suffix}")


# Read RTTM files generated by Pyannote into an array containing the speaker, start, and end of their speech in the audio
def load_diary(file):
	global speech_diary
	speech_diary = []
	with open(file, 'r', encoding='utf-8') as diary_file:
		for line in diary_file.read().strip().split('\n'):
			line_values = line.split(' ')
			speech_diary.append([line_values[7], float(line_values[3]), float(line_values[4])])
	total_speakers = len(set(line[0] for line in speech_diary))
	initialize_speakers(total_speakers)

def update_diary_timing():
	global speech_diary_adjusted
	# Time Shift the speech diary to be in line with the start time
	speech_diary_adjusted = [[int(line[0].split('_')[1]), line[1] + start_time, line[2]] for line in speech_diary]
	# adjust speech diary to crop only the bounded lines
	# speech_diary_adjusted = speech_diary_adjusted[find_nearest([line[1] for line in speech_diary_adjusted], start_time):find_nearest([line[1] for line in speech_diary_adjusted], end_line)]

# This function finds the closest index for the given value
def find_nearest(array, value):
	return (np.abs(np.asarray(array) - value)).argmin()

def initialize_speakers(speaker_count):
	global speakers
	speakers = [Voice.Voice(Voice.Voice.VoiceType.COQUI, name="Sample")]
	speakers[0].set_voice_params('tts_models/en/vctk/vits', 'p326') # p340
	speaker_options = speakers[0].list_speakers()
	for i in range(speaker_count):
		speakers.append(Voice.CoquiVoice([], f"Voice {i}"))
		speakers[i].set_voice_params('tts_models/en/vctk/vits', random.choice(speaker_options))
	return speakers

def post_process(file, source_start, source_end, adjust_volume=True):
	adjustment = match_rate(file, source_end-source_start)
	if adjust_volume:
		adjustment = match_volume(get_snippet(source_start, source_end), adjustment, adjustment)
	return adjustment

def find_nearest_speaker(sub):
	return speech_diary_adjusted[
		find_nearest(
			[diary_entry[1] for diary_entry in speech_diary_adjusted],
			sub.start
		)
	][0]

def run_dubbing(progress_hook=None):
	global dub_track
	total_errors = 0
	operation_start_time = time.process_time()
	empty_audio = AudioSegment.silent(total_duration * 1000, frame_rate=22050)
	status = ""
	# with concurrent.futures.ThreadPoolExecutor(max_workers=100) as pool:
	# 	tasks = [pool.submit(dub_task, sub, i) for i, sub in enumerate(subs_adjusted)]		
	# 	for future in concurrent.futures.as_completed(tasks):
	# 		pass
	for i, sub in enumerate(subs_adjusted):
		status = f"{i}/{len(subs_adjusted)}"
		try:
			line = dub_line_ram(sub)
			empty_audio = empty_audio.overlay(line, sub.start*1000)
		except Exception as e:
			print(e)
			total_errors += 1
		progress_hook(i, f"{status}: {sub.text}")
	dub_track = empty_audio.export(get_output_path(current_file, '-dubtrack.wav'), format="wav")
	progress_hook(i+1, "Mixing New Audio")
	mix_av(mixing_ratio=4)
	progress_hook(-1)
	print(f"TOTAL TIME TAKEN: {time.process_time() - operation_start_time}")
	# print(total_errors)

# This function was intended to run with multiprocessing, but Coqui won't play nice with that.
def dub_task(sub, i):
	print(f"{i}/{len(subs_adjusted)}")
	try:
		return dub_line_ram(sub)
		# empty_audio = empty_audio.overlay(line, sub.start*1000) 
	except Exception as e:
		print(e)
		with open(f"output/errors/{i}-rip.txt", 'w') as f:
			f.write(e)
		# total_errors += 1

# This may be used for multithreading?
def combine_segments():
	empty_audio = AudioSegment.silent(total_duration * 1000, frame_rate=22050)
	total_errors = 0
	for sub in subs_adjusted:
		print(f"{sub.index}/{len(subs_adjusted)}")
		try:
			segment = AudioSegment.from_file(f'output/files/{sub.index}.wav')
			empty_audio = empty_audio.overlay(segment, sub.start*1000)
		except:
			total_errors += 1
	empty_audio.export('new.wav')
	print(total_errors)

def sampleVoice(text, output=default_sample_path):
	play(AudioSegment.from_file(sampleSpeaker.speak(text, output)))

def match_rate(target, source_duration, destination_path=None, clamp_min=0, clamp_max=4):
	if destination_path == None:
		destination_path = target.split('.')[0] + '-timeshift.wav'
	duration = float(ffmpeg.probe(target)["format"]["duration"])
	rate = duration*1/source_duration
	rate = np.clip(rate, clamp_min, clamp_max)
	with WavReader(target) as reader:
		with WavWriter(destination_path, reader.channels, reader.samplerate) as writer:
			tsm = wsola(reader.channels, speed=rate)
			tsm.run(reader, writer)
	return destination_path

def match_rate_ram(target, source_duration, outpath=None, clamp_min=0.8, clamp_max=2.5):
	num_samples = len(target)
	target = target.reshape(1, num_samples)
	duration = num_samples / 22050
	rate = duration*1/source_duration
	rate = np.clip(rate, clamp_min, clamp_max)
	reader = ArrayReader(target)
	tsm = wsola(reader.channels, speed=rate)
	if not outpath:
		rate_adjusted = ArrayWriter(channels=1)
		tsm.run(reader, rate_adjusted)
		return rate_adjusted.data
	else:
		rate_adjusted = WavWriter(outpath, 1, 22050)
		tsm.run(reader, rate_adjusted)
		rate_adjusted.close()
		return outpath

def download_video(link, progress_hook=None):
	options = {
		'outtmpl': 'output/download.%(ext)s',
		'writesubtitles': True,
		"subtitlesformat": "srt",
		"progress_hooks": (progress_hook,)
	}
	with YoutubeDL(options) as ydl:
		info = ydl.extract_info(link)
		return ydl.prepare_filename(info), list(info["subtitles"].values())[0][-1]["filepath"]
	
def get_snippet(start, end):
	return current_audio[start*1000:end*1000]

def match_volume(source_snippet, target):
	ratio = source_snippet.rms / (target.rms | 1)
	# ratio = source_snippet.dBFS - target.dBFS
	adjusted_audio = target.apply_gain(ratio)
	# adjusted_audio = target + raio
	return adjusted_audio
	# adjusted_audio.export(output_path, format="wav")

def timecode_to_seconds(timecode):
	parts = list(map(float, timecode.split(':')))
	seconds = parts[-1]
	if len(parts) > 1:
		seconds += parts[-2] * 60
	if len(parts) > 2:
		seconds += parts[-3] * 3600
	return seconds

def seconds_to_timecode(seconds):
	hours = int(seconds // 3600)
	minutes = int((seconds % 3600) // 60)
	seconds = seconds % 60
	timecode = ""
	if hours:
		timecode += f"{hours}:"
	if minutes:
		timecode += f"{minutes}:" 
	timecode = f"{timecode}{seconds:05.2f}"
	return timecode


# This is like REALLY STINKY and DESPERATELY needs to be refactored... but i don't wanna right now
def load_video(video_path, progress_hook=None, callback=None):
	global subs, subs_adjusted, current_audio, total_duration, current_file
	sub_path = ""
	if video_path.startswith("http"):
		video_path, sub_path = download_video(video_path, progress_hook)
	current_file = video_path
	subs = subs_adjusted = load_subs(sub_path or video_path, get_output_path(current_file, '.srt'))
	current_audio = AudioSegment.from_file(video_path)
	total_duration = float(ffmpeg.probe(video_path)["format"]["duration"])
	time_change(0, total_duration)
	if callback: callback()

def time_change(start, end):
	global start_time, end_time, subs_adjusted
	start_time = start
	end_time = end
	# clamp the subs to the crop time specified
	start_line = find_nearest([sub.start for sub in subs], start)
	end_line = find_nearest([sub.start for sub in subs], end)
	subs_adjusted = subs[start_line:end_line]
	if speech_diary:
		update_diary_timing()

def crop_audio(file):
	# ffmpeg -i .\saiki.mkv -vn -ss 84 -to 1325 crop.wav
	output = get_output_path(file, "-crop.wav")
	(
		ffmpeg
		.input(file, ss=start_time, to=end_time)
		.output(output)
		.global_args('-loglevel', 'error')
		.global_args('-vn')
		.run(overwrite_output=True)
	)
	return output

def dub_line_ram(sub, output=True):
	output_path = get_output_path(str(sub.index), '.wav', path='files')
	tts_audio = speakers[sub.voice].speak(sub.text)
	rate_adjusted = match_rate_ram(tts_audio, sub.end-sub.start)
	data = rate_adjusted / np.max(np.abs(rate_adjusted))
	audio_as_int = (data * (2**15)).astype(np.int16).tobytes()
	segment = AudioSegment(
		audio_as_int,
		frame_rate=22050,
		sample_width=2,
		channels=1
	)
	# segment = AudioSegment.from_wav(output_path)
	result = segment #match_volume(get_snippet(sub.start, sub.end), segment)
	if output:
		result.export(output_path, format='wav')
	return result

def optimize_audio_diarization():
	crop = crop_audio(current_file)
	waveform, sample_rate = torchaudio.load(crop)
	# Apply noise reduction
	noise_reduce = T.Vad(sample_rate=sample_rate)
	clean_waveform = noise_reduce(waveform)
	
	# Normalize audio
	normalize = T.Resample(orig_freq=sample_rate, new_freq=sample_rate)
	normalized_waveform = normalize(clean_waveform)

	return normalized_waveform, sample_rate

def run_diarization():
	output = get_output_path(current_file, ".rttm")
	optimized, sample_rate = optimize_audio_diarization()
	diarization = pipeline({"waveform": optimized, "sample_rate": sample_rate})
	with open(output, "w") as rttm:
		diarization.write_rttm(rttm)
	load_diary(output)
	update_diary_timing()
	for sub in subs_adjusted:
		sub.voice = find_nearest_speaker(sub)


def mix_av(video_path=current_file, wav_path=get_output_path(current_file, '-dubtrack.wav'), mixing_ratio=6, output_path=get_output_path(current_file, '-dubbed.mkv')):
	input_video = ffmpeg.input(video_path)
	input_audio = input_video.audio
	input_wav = ffmpeg.input(wav_path).audio

	mixed_audio = ffmpeg.filter([input_audio, input_wav], 'amix', duration='first', weights=f"1 {mixing_ratio}")

	output = (
		# input_video['s']
		ffmpeg.output(input_video['v'], mixed_audio, output_path, vcodec="copy", acodec="aac")
		.global_args('-shortest')
	)
	ffmpeg.run(output, overwrite_output=True)

language_id = EncoderClassifier.from_hparams(source="speechbrain/lang-id-voxlingua107-ecapa", savedir="tmp")
def isnt_target_language(target="./output/video_snippet.wav", exclusion="English"):
	signal = language_id.load_audio(target)
	prediction = language_id.classify_batch(signal)
	return prediction[3][0].split(' ')[1] != exclusion

def filter_multilingual_subtiles(progress_hook=None):
	global subs_adjusted
	operation_start_time = time.process_time()
	multi_lingual_subs = []
	for i, sub in enumerate(subs_adjusted):
		progress_hook(i, f"{i}/{len(subs_adjusted)}: {sub.text}")
		snippet = get_snippet(sub.start, sub.end).export(get_output_path('video_snippet', '.wav'), format="wav")
		if isnt_target_language("output/video_snippet.wav"):
			multi_lingual_subs.append(sub)
	subs_adjusted = multi_lingual_subs
	progress_hook(-1, "done")
	print(f"TOTAL TIME TAKEN: {time.process_time() - operation_start_time}")

speakers = [Voice.Voice(Voice.Voice.VoiceType.COQUI, name="Sample")]
speakers[0].set_voice_params('tts_models/en/vctk/vits', 'p326') # p340
currentSpeaker = speakers[0]
sampleSpeaker = currentSpeaker

load_video("https://youtu.be/2rtt6MLvFmc?si=QAHlTSuz4HPE0IGN", print, print)
dub_line_ram(subs_adjusted[1], True)
# time_change(test_start_time, test_end_time)
# run_diarization()
# run_dubbing()

