# This file implements all the major globals & functions of the app. It's becoming unmaintainable and desperately needs to be refactored.

import ffmpeg
import numpy as np
import Voice
from pydub import AudioSegment
from pyannote.audio import Pipeline
import time
import concurrent.futures

from dub_line import load_subs
import random
import torchaudio
import torchaudio.transforms as T
from utils import get_output_path, find_nearest

# pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization@2.1", use_auth_token="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")

# Read RTTM files generated by Pyannote into an array containing the speaker, start, and end of their speech in the audio
def load_diary(file):
	global speech_diary
	speech_diary = []
	with open(file, 'r', encoding='utf-8') as diary_file:
		for line in diary_file.read().strip().split('\n'):
			line_values = line.split(' ')
			speech_diary.append([line_values[7], float(line_values[3]), float(line_values[4])])
	total_speakers = len(set(line[0] for line in speech_diary))
	initialize_speakers(total_speakers)

def update_diary_timing():
	global speech_diary_adjusted
	# Time Shift the speech diary to be in line with the start time
	speech_diary_adjusted = [[int(line[0].split('_')[1]), line[1] + start_time, line[2]] for line in speech_diary]
	# adjust speech diary to crop only the bounded lines
	# speech_diary_adjusted = speech_diary_adjusted[find_nearest([line[1] for line in speech_diary_adjusted], start_time):find_nearest([line[1] for line in speech_diary_adjusted], end_line)]

def initialize_speakers(speaker_count):
	global speakers
	speakers = [Voice.Voice(Voice.Voice.VoiceType.COQUI, name="Sample")]
	speakers[0].set_voice_params('tts_models/en/vctk/vits', 'p326') # p340
	speaker_options = speakers[0].list_speakers()
	for i in range(speaker_count):
		speakers.append(Voice.CoquiVoice([], f"Voice {i}"))
		speakers[i].set_voice_params('tts_models/en/vctk/vits', random.choice(speaker_options))
	return speakers

def find_nearest_speaker(sub):
	return speech_diary_adjusted[
		find_nearest(
			[diary_entry[1] for diary_entry in speech_diary_adjusted],
			sub.start
		)
	][0]

def run_dubbing(progress_hook=None):
	global dub_track
	total_errors = 0
	operation_start_time = time.process_time()
	empty_audio = AudioSegment.silent(total_duration * 1000, frame_rate=22050)
	status = ""
	# with concurrent.futures.ThreadPoolExecutor(max_workers=100) as pool:
	# 	tasks = [pool.submit(dub_task, sub, i) for i, sub in enumerate(subs_adjusted)]		
	# 	for future in concurrent.futures.as_completed(tasks):
	# 		pass
	for i, sub in enumerate(subs_adjusted):
		status = f"{i}/{len(subs_adjusted)}"
		try:
			line = dub_line_ram(sub)
			empty_audio = empty_audio.overlay(line, sub.start*1000)
		except Exception as e:
			print(e)
			total_errors += 1
		progress_hook(i, f"{status}: {sub.text}")
	dub_track = empty_audio.export(get_output_path(current_file, '-dubtrack.wav'), format="wav")
	progress_hook(i+1, "Mixing New Audio")
	mix_av(mixing_ratio=4)
	progress_hook(-1)
	print(f"TOTAL TIME TAKEN: {time.process_time() - operation_start_time}")
	# print(total_errors)

# This function was intended to run with multiprocessing, but Coqui won't play nice with that.
def dub_task(sub, i):
	print(f"{i}/{len(subs_adjusted)}")
	try:
		return dub_line_ram(sub)
		# empty_audio = empty_audio.overlay(line, sub.start*1000) 
	except Exception as e:
		print(e)
		with open(f"output/errors/{i}-rip.txt", 'w') as f:
			f.write(e)
		# total_errors += 1

# This may be used for multithreading?
def combine_segments():
	empty_audio = AudioSegment.silent(total_duration * 1000, frame_rate=22050)
	total_errors = 0
	for sub in subs_adjusted:
		print(f"{sub.index}/{len(subs_adjusted)}")
		try:
			segment = AudioSegment.from_file(f'output/files/{sub.index}.wav')
			empty_audio = empty_audio.overlay(segment, sub.start*1000)
		except:
			total_errors += 1
	empty_audio.export('new.wav')
	print(total_errors)


def dub_line_ram(sub, output=True):
	output_path = get_output_path(str(sub.index), '.wav', path='files')
	tts_audio = speakers[sub.voice].speak(sub.text)
	rate_adjusted = match_rate_ram(tts_audio, sub.end-sub.start)
	data = rate_adjusted / np.max(np.abs(rate_adjusted))
	audio_as_int = (data * (2**15)).astype(np.int16).tobytes()
	segment = AudioSegment(
		audio_as_int,
		frame_rate=22050,
		sample_width=2,
		channels=1
	)
	# segment = AudioSegment.from_wav(output_path)
	result = segment #match_volume(get_snippet(sub.start, sub.end), segment)
	if output:
		result.export(output_path, format='wav')
	return result

def optimize_audio_diarization():
	crop = crop_audio(current_file)
	waveform, sample_rate = torchaudio.load(crop)
	# Apply noise reduction
	noise_reduce = T.Vad(sample_rate=sample_rate)
	clean_waveform = noise_reduce(waveform)
	
	# Normalize audio
	normalize = T.Resample(orig_freq=sample_rate, new_freq=sample_rate)
	normalized_waveform = normalize(clean_waveform)

	return normalized_waveform, sample_rate

def run_diarization():
	output = get_output_path(current_file, ".rttm")
	optimized, sample_rate = optimize_audio_diarization()
	diarization = pipeline({"waveform": optimized, "sample_rate": sample_rate})
	with open(output, "w") as rttm:
		diarization.write_rttm(rttm)
	load_diary(output)
	update_diary_timing()
	for sub in subs_adjusted:
		sub.voice = find_nearest_speaker(sub)

def filter_multilingual_subtiles(progress_hook=None):
	global subs_adjusted
	operation_start_time = time.process_time()
	multi_lingual_subs = []
	for i, sub in enumerate(subs_adjusted):
		progress_hook(i, f"{i}/{len(subs_adjusted)}: {sub.text}")
		snippet = get_snippet(sub.start, sub.end).export(get_output_path('video_snippet', '.wav'), format="wav")
		if isnt_target_language("output/video_snippet.wav"):
			multi_lingual_subs.append(sub)
	subs_adjusted = multi_lingual_subs
	progress_hook(-1, "done")
	print(f"TOTAL TIME TAKEN: {time.process_time() - operation_start_time}")


