# This file just has an MVP function, a whole bunch of functions and testing things that'll be refactored.

import pyttsx3
import ffmpeg
import srt
# from TTS.api import TTS
import numpy as np
import re
import Voice
# # from pydub import AudioSegment
from mimic3_tts import tts as m3

# READ SUBS
def get_subs(file):
	output = f"output/{file.split('.')[0]}.srt"
	# (
	# 	ffmpeg
	# 	.input(file)
	# 	.output(output)
	# 	.run()
	# )
	with open(output, "r") as f:
		return list(srt.parse(f.read()))

# Read RTTM files generated by Pyannote into an array containing the speaker, start, and end of their speech in the audio
def read_diary(file):
	speech_diary = []
	with open(file, 'r') as diary_file:
		for line in diary_file.read().strip().split('\n'):
			line_values = line.split(' ')
			speech_diary.append([line_values[7], float(line_values[3]), float(line_values[4])])
	return speech_diary

# This function finds the closest index for the given value
def find_nearest(array, value):
	return (np.abs(np.asarray(array) - value)).argmin()

subs = get_subs("saiki.mkv")
speech_diary = read_diary("audio.rttm")

start_time = 94
end_time =  124 #1324
CLEANR = re.compile('<.*?>')

start_line = find_nearest([sub.start.total_seconds() for sub in subs], start_time)
end_line = find_nearest([sub.start.total_seconds() for sub in subs], end_time)
speech_diary_adjusted = [[line[0], line[1] + subs[start_line].start.total_seconds(), line[2]] for line in speech_diary]

# # Create unique speakers
total_speakers = len(set(line[0] for line in speech_diary)) # determine the total number of speakers in the diary
def initialize_speakers(speaker_count):
	speakers = []
	for i in range(total_speakers):
		speakers.append(Voice.SAPI5Voice([], f"Voice {i}"))
	return speakers
speakers = initialize_speakers(total_speakers)

speakers[1].set_voice_params(voice=1)
# speakers[2].set_voice_params(**{'pitch': 90})

total_duration = (end_time - start_time)*1000
# # empty_audio = AudioSegment.silent(duration=total_duration)
# # empty_audio = AudioSegment.from_file('saiki.mkv')

# # tts = Voice(Voice.VoiceType.COQUI, TTS.list_models()[8]) # 8 13) # {'model_path': '/home/tessa/Downloads/LibriTTS', 'config_path': '/home/tessa/Downloads/LibriTTS/config.json'})

# # Synth
def synth():
	for sub in subs[start_line:end_line]:
		text = re.sub(CLEANR, '', sub.content)
		current_speaker = int(speech_diary_adjusted[find_nearest([line[1] for line in speech_diary_adjusted], sub.start.total_seconds())][0].split('_')[1])
		current_speaker.set_speed(60*int((len(text.split(' ')) / (sub.end.total_seconds() - sub.start.total_seconds()))))
		file_name = f"files/{sub.index}.wav"
		current_speaker.speak(text, file_name)
		print(text)
		# empty_audio = empty_audio.overlay(AudioSegment.from_file(file_name), position=sub.start.total_seconds()*1000)
	# empty_audio.export("out.wav")

# # synth()
# # print('\n\nSPEAKERS\n', tts.speakers)
from TTS.api import TTS
# # LIST ALL COQUI Models	
# for (index, model) in enumerate(TTS.list_models()):
	# if '/en/' in model: print(index, model)
	# print(index, model)

# tts = TTS()

# tts.load_tts_model_by_name('tts_models/en/vctk/vits')
# for speaker in tts.speakers:
# 	tts.tts_to_file("flipping heck guys I need to awoo", speaker=speaker, file_path=f"output/{speaker}.wav", gpu=True)